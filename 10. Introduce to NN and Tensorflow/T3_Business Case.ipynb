{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_csv_data = np.loadtxt(\"udemy_data/Audiobooks_data.csv\", delimiter=',')\n",
    "feature = raw_csv_data[:,1:-1] #take the data start from index 1 till the end of index -1\n",
    "target = raw_csv_data[:,-1] #take the last index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balance the dataset\n",
    "You can also handle this case by under-sampling or over-sampling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_one_target = int(np.sum(target)) #the number of data with label 1 \n",
    "zero_target_count = 0\n",
    "index_to_remove = []\n",
    "\n",
    "for i in range(target.shape[0]): #shep (rowsxcols), so we need to take the row number\n",
    "    if target[i] == 0:\n",
    "        zero_target_count += 1\n",
    "        if zero_target_count > num_one_target: #if the number of 0 and 1 is equal, delete the remaining 0 data\n",
    "            index_to_remove.append(i) #by append the index to this array\n",
    "            \n",
    "feature_balance = np.delete(feature, index_to_remove, axis=0) #remove the row data feature\n",
    "target_balance = np.delete(target, index_to_remove, axis=0) #remove the target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize the input \n",
    "scale use to make the value of every feature not really different. Sometimes, I found the data with the value between 1-10 in feature A and 10000-1000000 in feature B. So we need to make the scale of feature A and B not realy different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "scale_input = preprocessing.scale(feature_balance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle the data\n",
    "we want to batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_indices = np.arange(scale_input.shape[0]) #create array from 0 - len scale_input\n",
    "np.random.shuffle(shuffled_indices)#random the value\n",
    "\n",
    "shuffled_input = scale_input[shuffled_indices]\n",
    "shuffled_target = target_balance[shuffled_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into train, validation, test\n",
    "You can also use train_test_split using sklearn library. How to split the data ? let say we split data into train and test with test_size = 0.1. Then split the train data into train and validation with test_size = 0.1. This is equal with the methid below (below is manual method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3579 447 448\n"
     ]
    }
   ],
   "source": [
    "samples_count = shuffled_input.shape[0] #get the len\n",
    "\n",
    "train_samples_count = int(0.8*samples_count)\n",
    "validation_count = int(0.1*samples_count)\n",
    "test_count = samples_count - train_samples_count - validation_count #the remaining data from train and validation\n",
    "\n",
    "train_inputs = shuffled_input[:train_samples_count]\n",
    "train_targets = shuffled_target[:train_samples_count]\n",
    "\n",
    "validation_input = shuffled_input[train_samples_count: validation_count+train_samples_count]\n",
    "validation_targets = shuffled_target[train_samples_count: validation_count+train_samples_count]\n",
    "\n",
    "test_input = shuffled_input[train_samples_count+validation_count:]\n",
    "test_target = shuffled_target[train_samples_count+validation_count:]\n",
    "\n",
    "print(train_samples_count, validation_count, test_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the dataset to .npz format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('Audiobooks_data_train', inputs=train_inputs, targets = train_targets)\n",
    "np.savez('Audiobooks_data_validation', inputs=validation_input, targets = validation_targets)\n",
    "np.savez('Audiobooks_data_test', inputs=test_input, targets = test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I have change the location for the data, so you can set your data location when doing np.load\n",
    "npz_train = np.load(\"udemy_data/Audiobooks_data_train.npz\")\n",
    "train_inputs, train_targets = npz_train['inputs'].astype(np.float), npz_train['targets'].astype(np.int)\n",
    "\n",
    "npz_val = np.load(\"udemy_data/Audiobooks_data_validation.npz\")\n",
    "val_inputs, val_targets = npz_val['inputs'].astype(np.float), npz_val['targets'].astype(np.int)\n",
    "\n",
    "npz_test = np.load(\"udemy_data/Audiobooks_data_test.npz\")\n",
    "test_inputs, test_targets = npz_test['inputs'].astype(np.float), npz_test['targets'].astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "36/36 - 0s - loss: 0.5163 - accuracy: 0.7206 - val_loss: 0.4042 - val_accuracy: 0.7987\n",
      "Epoch 2/100\n",
      "36/36 - 0s - loss: 0.3865 - accuracy: 0.7882 - val_loss: 0.3660 - val_accuracy: 0.7987\n",
      "Epoch 3/100\n",
      "36/36 - 0s - loss: 0.3582 - accuracy: 0.8041 - val_loss: 0.3626 - val_accuracy: 0.8098\n",
      "Epoch 4/100\n",
      "36/36 - 0s - loss: 0.3535 - accuracy: 0.7946 - val_loss: 0.3436 - val_accuracy: 0.8166\n",
      "Epoch 5/100\n",
      "36/36 - 0s - loss: 0.3472 - accuracy: 0.8150 - val_loss: 0.3492 - val_accuracy: 0.8031\n",
      "Epoch 6/100\n",
      "36/36 - 0s - loss: 0.3461 - accuracy: 0.8086 - val_loss: 0.3389 - val_accuracy: 0.8054\n",
      "Epoch 7/100\n",
      "36/36 - 0s - loss: 0.3418 - accuracy: 0.8083 - val_loss: 0.3666 - val_accuracy: 0.8121\n",
      "Epoch 8/100\n",
      "36/36 - 0s - loss: 0.3358 - accuracy: 0.8089 - val_loss: 0.3572 - val_accuracy: 0.7919\n",
      "Epoch 9/100\n",
      "36/36 - 0s - loss: 0.3391 - accuracy: 0.8092 - val_loss: 0.3501 - val_accuracy: 0.8031\n",
      "Epoch 10/100\n",
      "36/36 - 0s - loss: 0.3287 - accuracy: 0.8259 - val_loss: 0.3403 - val_accuracy: 0.8054\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e324b7c248>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = 10 #there are 10 feature that we have\n",
    "output_size = 2 #just 0 and 1\n",
    "hidden_layer_size = 100 #enough for complex case\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "batch_size=100\n",
    "epochs_size = 100\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=4)#early stopping method\n",
    "\n",
    "model.fit(train_inputs, train_targets,\n",
    "         batch_size = batch_size, \n",
    "         epochs = epochs_size,\n",
    "         callbacks = [early_stopping],\n",
    "         validation_data=(val_inputs, val_targets), verbose= 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 829us/step - loss: 0.3083 - accuracy: 0.8304\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_inputs, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.31 and Test accuracy: 83.04% \n"
     ]
    }
   ],
   "source": [
    "print(\"Test loss: {0:.2f} and Test accuracy: {1:.2f}% \".format(test_loss,test_accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_list = [\"id\", \"book_length_overall\", \"book_length_avg\",\"price_overall\",\"prive_avg\",\"review\",\"review_scale\",\"minutes_listened\",\"completion\",\"support_request\",\"last_visit_minutes\",\"target\"]\n",
    "df = pd.read_csv(\"udemy_data/Audiobooks_data.csv\", names=header_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14084 entries, 0 to 14083\n",
      "Data columns (total 12 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   id                   14084 non-null  int64  \n",
      " 1   book_length_overall  14084 non-null  float64\n",
      " 2   book_length_avg      14084 non-null  int64  \n",
      " 3   price_overall        14084 non-null  float64\n",
      " 4   prive_avg            14084 non-null  float64\n",
      " 5   review               14084 non-null  int64  \n",
      " 6   review_scale         14084 non-null  float64\n",
      " 7   minutes_listened     14084 non-null  float64\n",
      " 8   completion           14084 non-null  float64\n",
      " 9   support_request      14084 non-null  int64  \n",
      " 10  last_visit_minutes   14084 non-null  int64  \n",
      " 11  target               14084 non-null  int64  \n",
      "dtypes: float64(6), int64(6)\n",
      "memory usage: 1.3 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = df['target'].value_counts().plot(kind='bar', figsize=(5,5), color ='orange')\n",
    "for p in ax.patches:\n",
    "    ax.annotate(str(p.get_height()), (p.get_x() * 1.005, p.get_height() * 1.005))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balance the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2237\n",
       "0    2237\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "fraud = df[df['target'] == 0]\n",
    "not_fraud = df[df['target'] == 1]\n",
    "\n",
    "# upsample minority\n",
    "fraud_upsampled = resample(fraud,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=len(not_fraud), # match number in majority class\n",
    "                          random_state=27) # reproducible results\n",
    "\n",
    "\n",
    "# combine majority and upsampled minority\n",
    "df_balance = pd.concat([not_fraud, fraud_upsampled])\n",
    "\n",
    "# check new class counts\n",
    "df_balance.target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balance = df_balance.sample(frac=1) #shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature = df_balance.drop(['id','target'], axis=1) #get the feature and target\n",
    "df_label = df_balance['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler().fit(df_feature)\n",
    "df_feature_std = scaler.transform(df_feature)\n",
    "\n",
    "header_list = [\"book_length_overall\", \"book_length_avg\",\"price_overall\",\"prive_avg\",\"review\",\"review_scale\",\"minutes_listened\",\"completion\",\"support_request\",\"last_visit_minutes\"]\n",
    "df_feature_std = pd.DataFrame(data=df_feature_std,columns=header_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_length_overall</th>\n",
       "      <th>book_length_avg</th>\n",
       "      <th>price_overall</th>\n",
       "      <th>prive_avg</th>\n",
       "      <th>review</th>\n",
       "      <th>review_scale</th>\n",
       "      <th>minutes_listened</th>\n",
       "      <th>completion</th>\n",
       "      <th>support_request</th>\n",
       "      <th>last_visit_minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.179810</td>\n",
       "      <td>3.595398</td>\n",
       "      <td>-0.363783</td>\n",
       "      <td>1.138733</td>\n",
       "      <td>-0.453437</td>\n",
       "      <td>0.007525</td>\n",
       "      <td>-0.391367</td>\n",
       "      <td>-0.382429</td>\n",
       "      <td>-0.186945</td>\n",
       "      <td>2.310627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.197922</td>\n",
       "      <td>0.365864</td>\n",
       "      <td>0.179295</td>\n",
       "      <td>-0.076941</td>\n",
       "      <td>-0.453437</td>\n",
       "      <td>0.007525</td>\n",
       "      <td>-0.391367</td>\n",
       "      <td>-0.382429</td>\n",
       "      <td>-0.186945</td>\n",
       "      <td>0.774558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.438329</td>\n",
       "      <td>0.986928</td>\n",
       "      <td>0.013922</td>\n",
       "      <td>0.891348</td>\n",
       "      <td>2.205379</td>\n",
       "      <td>1.476595</td>\n",
       "      <td>-0.391367</td>\n",
       "      <td>-0.382429</td>\n",
       "      <td>5.888549</td>\n",
       "      <td>1.272158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.107088</td>\n",
       "      <td>-0.255200</td>\n",
       "      <td>0.101713</td>\n",
       "      <td>-0.134614</td>\n",
       "      <td>-0.453437</td>\n",
       "      <td>0.007525</td>\n",
       "      <td>-0.391367</td>\n",
       "      <td>-0.382429</td>\n",
       "      <td>-0.186945</td>\n",
       "      <td>-0.761511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.765579</td>\n",
       "      <td>-0.752051</td>\n",
       "      <td>0.616208</td>\n",
       "      <td>0.247846</td>\n",
       "      <td>-0.453437</td>\n",
       "      <td>0.007525</td>\n",
       "      <td>-0.391367</td>\n",
       "      <td>-0.382429</td>\n",
       "      <td>1.838220</td>\n",
       "      <td>0.460854</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   book_length_overall  book_length_avg  price_overall  prive_avg    review  \\\n",
       "0             0.179810         3.595398      -0.363783   1.138733 -0.453437   \n",
       "1             1.197922         0.365864       0.179295  -0.076941 -0.453437   \n",
       "2            -0.438329         0.986928       0.013922   0.891348  2.205379   \n",
       "3             0.107088        -0.255200       0.101713  -0.134614 -0.453437   \n",
       "4            -0.765579        -0.752051       0.616208   0.247846 -0.453437   \n",
       "\n",
       "   review_scale  minutes_listened  completion  support_request  \\\n",
       "0      0.007525         -0.391367   -0.382429        -0.186945   \n",
       "1      0.007525         -0.391367   -0.382429        -0.186945   \n",
       "2      1.476595         -0.391367   -0.382429         5.888549   \n",
       "3      0.007525         -0.391367   -0.382429        -0.186945   \n",
       "4      0.007525         -0.391367   -0.382429         1.838220   \n",
       "\n",
       "   last_visit_minutes  \n",
       "0            2.310627  \n",
       "1            0.774558  \n",
       "2            1.272158  \n",
       "3           -0.761511  \n",
       "4            0.460854  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feature_std.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Split to train, validation, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_remain, y_train, y_remain = train_test_split(df_feature_std, df_label, test_size=0.2)\n",
    "X_validation, X_test, y_validation, y_test = train_test_split(X_remain, y_remain, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3579, 10) (447, 10) (448, 10)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_validation.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data to .np format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('Audiobooks_pandas_train', inputs=X_train, targets = y_train)\n",
    "np.savez('Audiobooks_pandas_validation', inputs=X_validation, targets = y_validation)\n",
    "np.savez('Audiobooks_pandas_test', inputs=X_test, targets = y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "npz_train = np.load(\"udemy_data/Audiobooks_pandas_train.npz\")\n",
    "train_inputs, train_targets = npz_train['inputs'].astype(np.float), npz_train['targets'].astype(np.int)\n",
    "\n",
    "npz_val = np.load(\"udemy_data/Audiobooks_pandas_validation.npz\")\n",
    "val_inputs, val_targets = npz_val['inputs'].astype(np.float), npz_val['targets'].astype(np.int)\n",
    "\n",
    "npz_test = np.load(\"udemy_data/Audiobooks_pandas_test.npz\")\n",
    "test_inputs, test_targets = npz_test['inputs'].astype(np.float), npz_test['targets'].astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "36/36 - 0s - loss: 0.5245 - accuracy: 0.7259 - val_loss: 0.3956 - val_accuracy: 0.8121\n",
      "Epoch 2/100\n",
      "36/36 - 0s - loss: 0.4060 - accuracy: 0.7860 - val_loss: 0.3590 - val_accuracy: 0.8210\n",
      "Epoch 3/100\n",
      "36/36 - 0s - loss: 0.3876 - accuracy: 0.7935 - val_loss: 0.3538 - val_accuracy: 0.8210\n",
      "Epoch 4/100\n",
      "36/36 - 0s - loss: 0.3785 - accuracy: 0.8005 - val_loss: 0.3573 - val_accuracy: 0.8076\n",
      "Epoch 5/100\n",
      "36/36 - 0s - loss: 0.3683 - accuracy: 0.8061 - val_loss: 0.3350 - val_accuracy: 0.8277\n",
      "Epoch 6/100\n",
      "36/36 - 0s - loss: 0.3719 - accuracy: 0.8041 - val_loss: 0.3396 - val_accuracy: 0.8210\n",
      "Epoch 7/100\n",
      "36/36 - 0s - loss: 0.3666 - accuracy: 0.7983 - val_loss: 0.3207 - val_accuracy: 0.8255\n",
      "Epoch 8/100\n",
      "36/36 - 0s - loss: 0.3755 - accuracy: 0.7997 - val_loss: 0.3340 - val_accuracy: 0.8255\n",
      "Epoch 9/100\n",
      "36/36 - 0s - loss: 0.3689 - accuracy: 0.8053 - val_loss: 0.3313 - val_accuracy: 0.8345\n",
      "Epoch 10/100\n",
      "36/36 - 0s - loss: 0.3677 - accuracy: 0.8053 - val_loss: 0.3232 - val_accuracy: 0.8434\n",
      "Epoch 11/100\n",
      "36/36 - 0s - loss: 0.3645 - accuracy: 0.8055 - val_loss: 0.3273 - val_accuracy: 0.8277\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1724a0e5908>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = 10 #there are 10 feature that we have\n",
    "output_size = 2 #just 0 and 1\n",
    "hidden_layer_size = 100 #enough for complex case\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "batch_size=100\n",
    "epochs_size = 100\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=4)#early stopping method\n",
    "\n",
    "model.fit(train_inputs, train_targets,\n",
    "         batch_size = batch_size, \n",
    "         epochs = epochs_size,\n",
    "         callbacks = [early_stopping],\n",
    "         validation_data=(val_inputs, val_targets), verbose= 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 992us/step - loss: 0.3740 - accuracy: 0.7924\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_inputs, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.37 and Test accuracy: 79.24% \n"
     ]
    }
   ],
   "source": [
    "print(\"Test loss: {0:.2f} and Test accuracy: {1:.2f}% \".format(test_loss,test_accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ========== Using Sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "params = {'min_samples_leaf':[1,2,3],\n",
    "          'min_samples_split':[2,3,4,5],\n",
    "          'min_weight_fraction_leaf':[0.0,0.04,0.07],\n",
    "          'n_estimators':[115,125,135],\n",
    "          'criterion':['gini','entropy'],\n",
    "          'class_weight':['None','balanced'],\n",
    "          'max_features':['auto', 'sqrt', 'log2']}\n",
    "model = RandomForestClassifier()\n",
    "gsc = GridSearchCV(model, params, scoring='roc_auc', cv=5, return_train_score=True, n_jobs = 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestClassifier(), n_jobs=10,\n",
       "             param_grid={'class_weight': ['None', 'balanced'],\n",
       "                         'criterion': ['gini', 'entropy'],\n",
       "                         'max_features': ['auto', 'sqrt', 'log2'],\n",
       "                         'min_samples_leaf': [1, 2, 3],\n",
       "                         'min_samples_split': [2, 3, 4, 5],\n",
       "                         'min_weight_fraction_leaf': [0.0, 0.04, 0.07],\n",
       "                         'n_estimators': [115, 125, 135]},\n",
       "             return_train_score=True, scoring='roc_auc')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'auto', 'min_samples_leaf': 3, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 125}\n",
      "0.9170593401993612\n"
     ]
    }
   ],
   "source": [
    "print(gsc.best_params_)\n",
    "print(gsc.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(class_weight = 'balanced',\n",
    "                               criterion = 'gini', \n",
    "                               max_features = 'auto',\n",
    "                               min_samples_leaf = 3, \n",
    "                               min_samples_split = 2, \n",
    "                               min_weight_fraction_leaf = 0.0, \n",
    "                               n_estimators = 125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(class_weight='balanced', min_samples_leaf=3,\n",
       "                       n_estimators=125)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8714724783459067"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8098434004474273"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#validation test\n",
    "from sklearn.metrics import accuracy_score\n",
    "valid_pred = model.predict(X_validation)\n",
    "accuracy_score(valid_pred, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7834821428571429"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred = model.predict(X_test)\n",
    "accuracy_score(test_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alphabet hackerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input number: 7\n",
      "------------g------------\n",
      "----------g-f-g----------\n",
      "--------g-f-e-f-g--------\n",
      "------g-f-e-d-e-f-g------\n",
      "----g-f-e-d-c-d-e-f-g----\n",
      "--g-f-e-d-c-b-c-d-e-f-g--\n",
      "g-f-e-d-c-b-a-b-c-d-e-f-g\n",
      "--g-f-e-d-c-b-c-d-e-f-g--\n",
      "----g-f-e-d-c-d-e-f-g----\n",
      "------g-f-e-d-e-f-g------\n",
      "--------g-f-e-f-g--------\n",
      "----------g-f-g----------\n",
      "------------g------------\n"
     ]
    }
   ],
   "source": [
    "alphabet = ['a','b','c','d','e','f','g','h','i','j']\n",
    "number = int(input(\"Input number: \"))\n",
    "dash = number\n",
    "sentences = []\n",
    "for line in range(number):\n",
    "    sentence = \"\"\n",
    "    #first dash\n",
    "    for dash_word in range((dash-1)*2):\n",
    "        sentence += \"-\"\n",
    "    \n",
    "    #letter\n",
    "    letter = alphabet[dash-1:number]\n",
    "    for i in range(len(letter),0,-1):\n",
    "        sentence += letter[i-1]\n",
    "        if i != 1:\n",
    "            sentence += \"-\"\n",
    "    if len(letter) != 1:\n",
    "        sentence += \"-\"\n",
    "    for i in range(1,len(letter)):\n",
    "        sentence += letter[i]\n",
    "        if i != len(letter)-1:\n",
    "            sentence += \"-\"\n",
    "        \n",
    "    #second dash\n",
    "    for dash_word in range((dash-1)*2):\n",
    "        sentence += \"-\"\n",
    "    dash = dash - 1\n",
    "    sentences.append(sentence)\n",
    "    \n",
    "#load data\n",
    "for i in sentences:\n",
    "    print(i)\n",
    "    \n",
    "for i in range(len(sentences)-2,-1,-1):\n",
    "    print(sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
